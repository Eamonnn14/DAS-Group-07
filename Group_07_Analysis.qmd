---
title: "DAS Group Project 2"
author: "Group 7"
number-sections: true
format: 
  html:
    embed-resources: true
    code-tools: true
  pdf: 
    fig-pos: "H"
editor_options: 
  chunk_output_type: console
execute:
  echo: false
  eval: true
  warning: false
  message: false
---

# Introduction

```{r}
#| label: libraries
library(ggplot2)
library(tidyverse)
library(gt)
library(patchwork)
library(gridExtra)
library(moderndive)
library(GGally)
library(corrplot)
library(caret)
library(pROC)
```

You can add options to executable code like this

```{r}
#| label: data
data <- read.csv("/Users/ziluwang/Documents/GitHub/DAS-Project2-Group7/dataset07.csv", na.strings = 'NA')
```

```{r}
# Check for missing values
colSums(is.na(data))
```

```{r}
# Data wrangling
data$length[is.na(data$length)] <- median(data$length, na.rm = TRUE)
# Creating a new binary variable
data$above_7 <- ifelse(data$rating > 7, 1, 0)
```

# EDA

```{r}
glimpse(data)
```

```{r}
# Statistical Summary
summary(data)
```

```{r}
data$length_log <- log1p(data$length)
data$votes_log <- log1p(data$votes)
```

```{r}
# Calculate the proportion of outliers for each numeric variable
# Defining the function to calculate the proportion of outliers
calculate_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  num_outliers <- sum(x < lower_bound | x > upper_bound, na.rm = TRUE)
  total_values <- sum(!is.na(x))
  proportion_outliers <- num_outliers / total_values
  return(proportion_outliers)
}

# Apply the function only to 'length', 'budget', and 'votes' columns
selected_columns <- c("length", "budget", "votes")
outlier_proportions <- sapply(data[selected_columns], calculate_outliers)

# Convert the proportions to a data frame for easier reading
outlier_table <- as.data.frame(t(outlier_proportions), row.names = "Proportion of Outliers")
outlier_table
```

```{r}
# Calculate the proportion of outliers for each numeric variable
# Defining the function to calculate the proportion of outliers
calculate_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  num_outliers <- sum(x < lower_bound | x > upper_bound, na.rm = TRUE)
  total_values <- sum(!is.na(x))
  proportion_outliers <- num_outliers / total_values
  return(proportion_outliers)
}

# Apply the function only to 'length', 'budget', and 'votes' columns
selected_columns <- c("length_log", "budget", "votes_log")
outlier_proportions <- sapply(data[selected_columns], calculate_outliers)

# Convert the proportions to a data frame for easier reading
outlier_table <- as.data.frame(t(outlier_proportions), row.names = "Proportion of Outliers")
outlier_table
```

```{r}
calculate_and_print_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  outliers <- x[x < lower_bound | x > upper_bound]
  return(outliers)
}

# Apply the modified function only to 'length', 'budget', and 'votes' columns
selected_columns <- c("length", "budget", "votes")
list_outliers <- lapply(data[selected_columns], calculate_and_print_outliers)

# Print the actual outliers for each variable
list_outliers
```

```{r}
# List of numeric variables
numeric_vars <- c("year", "length", "budget", "votes")

# Titles and x-axis labels for the histograms
titles <- c("Distribution of Years", "Distribution of Film Lengths",
            "Distribution of Budgets", "Distribution of Votes", "Distribution of Ratings")
x_labels <- c("Year", "Length (minutes)", "Budget (millions $)", "Votes", "Rating")

# Loop through numeric variables to create histograms
par(mfrow = c(2, 2)) # Adjust grid layout based on number of variables
for (i in 1:length(numeric_vars)) {
  hist(data[[numeric_vars[i]]], main = titles[i], xlab = x_labels[i], border = 'white')
}
par(mfrow = c(1, 1)) # Reset to default layout
```

```{r}
# List of numeric variables
numeric_vars <- c("year", "length_log", "budget", "votes_log")

# Titles and x-axis labels for the histograms
titles <- c("Distribution of Years", "Distribution of log(Film Lengths)",
            "Distribution of Budgets", "Distribution of log(Votes)")
x_labels <- c("Year", "Length (minutes)", "Budget (millions $)", "Votes", "Rating")

# Loop through numeric variables to create histograms
par(mfrow = c(2, 2)) # Adjust grid layout based on number of variables
for (i in 1:length(numeric_vars)) {
  hist(data[[numeric_vars[i]]], main = titles[i], xlab = x_labels[i], border = 'white')
}
par(mfrow = c(1, 1)) # Reset to default layout
```

```{r}
numeric_vars <- c("year", "length", "budget", "votes")
# Set up plotting area
par(mfrow = c(2, 2))  # Adjust dimensions as necessary based on the number of variables

# Loop through numeric variables to create boxplots
for (var in numeric_vars) {
    # Create boxplot for each numeric variable
    boxplot(data[[var]], main = paste("Distribution of", var), ylab = var)
}

# Reset plotting area to default
par(mfrow = c(1, 1))
```

```{r}
# Set up the layout for multiple plots (1 row, 2 columns)
par(mfrow = c(1, 2))  # Adjust layout as needed

# Bar plot for genre
genre_counts <- table(data$genre)
barplot(genre_counts, main = "Film Counts by Genre", xlab = "Genre", ylab = "Count", las = 2)

# Bar plot for above_7
above7_counts <- table(data$above_7)
barplot(above7_counts, main = "Film Counts by Above 7", xlab = "Above 7", ylab = "Count", las = 2)

# Reset to default layout
par(mfrow = c(1, 1))
```

```{r}
# Pairwise correlation between numeric variables
numeric_data <- dplyr::select(data, -film_id, -genre, -above_7, -rating)  # Remove non-numeric and unnecessary columns for correlation
cor_matrix <- cor(numeric_data, use = "complete.obs")  # Compute correlation matrix
corrplot(cor_matrix, type = "upper", order = "hclust", 
         tl.cex = 0.6, tl.col = "black", addCoef.col = "blue") 
```

```{r}
# Set up the layout for multiple plots
par(mfrow = c(2, 2))  # Adjust as necessary based on the number of variables

# Loop through numeric variables to create boxplots
for (var in numeric_vars) {
    formula = as.formula(paste(var, "~ above_7"))  # Construct formula for plotting
    boxplot(formula, data = data,
            main = paste(var, "vs. Above_7"),
            xlab = "Above 7", ylab = var)
}

# Reset the layout
par(mfrow = c(1, 1))
```

```{r}
# Recalculate proportions if necessary
genre_counts <- table(data$above_7, data$genre)
genre_proportions <- prop.table(genre_counts, 2)  # Calculate row-wise proportions

# Create the barplot
barplot(genre_proportions, legend = TRUE,
        main = "Proportion of Ratings Above 7 by Genre",
        xlab = "Genre", ylab = "Proportion",
        names.arg = colnames(genre_counts))  # Use column names of the original counts table
```

```{r}
# Boxplot of length grouped by genre
boxplot(length ~ genre, data = data,
        main = "Boxplot of Movie Length by Genre",
        xlab = "Genre", ylab = "Length",
        las = 2)  # Makes genre labels perpendicular for better readability

# Boxplot of budget grouped by genre
boxplot(budget ~ genre, data = data,
        main = "Boxplot of Movie Budget by Genre",
        xlab = "Genre", ylab = "Budget (millions $)",
        las = 2)  # Makes genre labels perpendicular for better readability

# Boxplot of budget grouped by genre
boxplot(votes_log ~ genre, data = data,
        main = "Boxplot of Log(Votes) by Genre",
        xlab = "Genre", ylab = "Log(Votes)",
        las = 2)  # Makes genre labels perpendicular for better readability
```

# Formal Analysis

```{r}
# Remove unwanted columns from dataset
data_clean <- dplyr::select(data, -film_id, -rating)
```

```{r}
# split train and test dataset 
set.seed(123)  # for reproducibility
index <- createDataPartition(data_clean$above_7, p = .70, list = FALSE)
train_data <- data_clean[index, ]
test_data <- data_clean[-index, ]
```

```{r}
# Define models
# Original Model
glm_model_orig <- glm(above_7 ~ year + length + budget + votes + genre, 
                      family = binomial, data = train_data)

# Full Model with Log Transformation
glm_model_log <- glm(above_7 ~ year + length_log + budget + votes_log + genre, 
                     family = binomial, data = train_data)

# Model without Year
glm_model_no_year <- glm(above_7 ~ length_log + budget + votes_log + genre, 
                        family = binomial, data = train_data)

# Model without Year and Votes_log
glm_model_no_year_votes <- glm(above_7 ~ length_log + budget + genre, 
                               family = binomial, data = train_data)
```

```{r}
# Modified evaluate_model function to return metrics
evaluate_model <- function(model, test_data) {
  predictions <- predict(model, test_data, type = "response")
  predicted_class <- ifelse(predictions > 0.32, 1, 0)  # Classification threshold at 0.32
  conf_matrix <- confusionMatrix(factor(predicted_class), factor(test_data$above_7))
  roc_response <- roc(response = test_data$above_7, predictor = predictions)
  
  # Compile performance metrics
  metrics <- list(
    Accuracy = conf_matrix$overall['Accuracy'],
    Sensitivity = conf_matrix$byClass['Sensitivity'],
    Specificity = conf_matrix$byClass['Specificity'],
    AUC = auc(roc_response), 
    BIC = BIC(model)
  )
  
  return(metrics)  # Return metrics for storage
}
# Assuming your models are named and your test_data is ready
# Store metrics in a structured way
metrics_list <- list()
metrics_list[['Full Model']] <- evaluate_model(glm_model_orig, test_data)
metrics_list[['Full model with Log']] <- evaluate_model(glm_model_log, test_data)
metrics_list[['Model without Year']] <- evaluate_model(glm_model_no_year, test_data)
metrics_list[['Model without Year and Votes']] <- evaluate_model(glm_model_no_year_votes, test_data)

# Compile metrics into a summary table
summary_table <- sapply(metrics_list, function(x) sapply(x, function(y) y))  # Collect metrics
summary_table <- t(summary_table)  # Transpose to make rows correspond to models
summary_table <- round(summary_table, 4)  # Round for readability

# Setting the column names if they are not automatically set
colnames(summary_table) <- c("Accuracy", "Sensitivity", "Specificity", "AUC", "BIC")

# Print the summary table
summary_table
```

```{r}
summary(glm_model_no_year)
```

From the summary of the Generalized Linear Model (GLM) analysis, we can draw the following findings and conclusions:

1.  **Length of Movies (length_log)**: There is a significant negative relationship between the log-transformed length of movies and their likelihood of being rated above 7. This suggests that longer movies are less likely to receive high ratings, potentially indicating viewer preferences for shorter films or perhaps an association with certain film types or genres that are longer but less popular.

2.  **Budget (budget)**: The budget of a movie shows a significant positive association with the likelihood of being rated above 7. This might imply that higher-budget movies, which can afford better production quality, actors, and marketing, are more likely to be well-received by audiences.

3.  **Votes (votes_log)**: The log-transformed number of votes is positively correlated with a movie being rated above 7. This indicates that movies that engage more viewers to vote are likely to have higher ratings. It could reflect higher viewer engagement or broader appreciation.

4.  **Genre Differences**:

    -   **Animation**: Compared to the baseline genre, animation films are significantly less likely to be rated above 7. This could reflect specific audience preferences or the standards by which animation is judged.
    -   **Comedy and Documentary**: These genres show a significant positive association with higher ratings, suggesting they are generally well-received or cater to specific audience segments that rate them favorably.
    -   **Drama**: Dramas are less likely to score above 7, indicating perhaps a critical standard or audience expectation that is harder to meet.
    -   **Romance and Short**: These genres do not show significant effects, possibly due to a smaller sample size, less variation in ratings, or other model limitations.

5.  **Model Performance**:

    -   The model has demonstrated high accuracy (88.55%), indicating a strong ability to classify films correctly as having ratings above or below 7. This level of accuracy suggests that the variables chosen, including movie length, budget, number of votes, and genre, are significant indicators of a film's rating performance.
    -   Sensitivity (88.71%) and specificity (88.21%) values are both high, showing that the model is proficient not only in identifying true positives (correctly predicting films rated above 7) but also in recognizing true negatives (correctly predicting films not rated above 7). This balance is crucial for ensuring the model's reliability across different film scenarios.
    -   The AUC (Area Under the Curve) of 0.9457 signifies excellent model discrimination ability, meaning it has a high capability in distinguishing between films rated above 7 and those that are not.

6.  **Model Fit and Data Quality**:

    -   The substantial gap between null and residual deviance indicates that the model fits the data well beyond a mere intercept-only model.
    -   However, the BIC of 950.4586, while providing a measure of model quality, suggests room for improvement or simplification, considering it penalizes complex models. The relatively high BIC compared to the model's predictive success (e.g., AUC) indicates that while the model is effective, it could be made more efficient or tailored.

7.  **Practical Implications**:

    -   Filmmakers and producers can leverage insights from this model, particularly around film length, budget, and targeted genre, to optimize their projects for higher audience ratings.
    -   The significant predictors offer a blueprint for aligning movie projects with characteristics correlated with success, though considerations of artistic intent and narrative integrity remain paramount.

8.  **Further Research and Limitations**:

    -   The disparities observed in genre impacts necessitate deeper investigation, potentially requiring broader datasets to ensure nuanced understandings.
    -   While the GLM offers robust insights, it's essential to remember that correlation does not guarantee causation; additional factors not included in the model may influence movie ratings.
    -   Future research should address the data limitations, particularly for underrepresented genres, and explore external factors beyond the scope of the current model to provide a more comprehensive understanding.

**Conclusions**: - The results suggest that specific attributes associated with movies, such as their duration, budget, and genre, significantly influence their ratings. - However, the effect of genre on movie ratings can vary widely, indicating that audience preferences and perceptions can differ markedly between different types of films. - The significant predictors in this model can be leveraged by filmmakers and producers to align their projects more closely with attributes associated with higher-rated films. However, it's essential to approach these findings with a nuanced understanding that correlation does not imply causation, and other unmeasured factors could also be influencing movie ratings. - The anomalies observed for certain genres highlight the need for further investigation, potentially with a larger or more balanced dataset, to understand these relationships better.

Overall, while log transformations and GLM have provided meaningful insights, it's crucial to consider these findings within the broader context of movie production and audience reception, and where possible, to validate these conclusions with additional data or through experimental approaches.
